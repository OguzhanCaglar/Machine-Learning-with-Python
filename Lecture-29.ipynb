{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture-29\n"
     ]
    }
   ],
   "source": [
    "print(\"Lecture-29\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geri Beslemeli Ağlarda Çıktı Üretimi\n",
    "\n",
    "Biz şimdiye kadar örneklerimizde gelecek kestirimi yapmaya çalıştık. Ancak bazn bu gelecek tahminleri çıktı üretimleri için de kullaınlabilmektedir. Özellikle metinlerin üretiminde 2015 yılından itibaren LSTM tabanlı geri beslemeli ağlar kullanılmaya başlanmıştır.\n",
    "\n",
    "Gerçekleştirme Adımları:\n",
    "\n",
    "1) Üretilecek metni belli bir kişinin tarzına uydurmak için o kişinin yazıları elde edilir.\n",
    "\n",
    "2) Sonra bir geri beslemeli sinir ağı oluştururarak sinir ağı yazılarla eğitilir. Burada karakter temelli bir çıktı üretiminin söz konusu olduğunu düşündelim. Bunun için seçilen kişiye ilişkin yazıların belli miktradaki karakterden oluşan kısmları - örneğin 32 karakter olduğunu düşünelim - ve bu kusumdan sonra gelen karakterler eğitim amacıyla toplanacaktır. Böylece bu işlemden 32 karakterden oıluşan bir yazı parçaları ile (dataset_x), 1 karakterden oluşan sonuç değerleri (dataset_y) elde edilecektir. \n",
    "\n",
    "3) Geri beslemeli bir yapay sinir ağı modeli kurularak ağ oluşturulan veri kümesiyle eğitilir. Geri besleme için genelllikle LSTM modeli tercih edilmektedir.\n",
    "\n",
    "4) Artık elimizde 32 karakterini girdi olarak verdiğimiz bir metnin 33. karkaterini tahmin edebilen bir geri beslemeli yapay sinir ağı modeli vardır. Bu durumda biz 32 karakterlik bir yazıyı başlangıç yazısı olarak verirsek ağdan bunun 33. karakterini elde edebiliriz. Bunu sonra yine bu 33 karakterli yazının son 32 karakterini alarak yeni bir karakter elde ederiz. Bu işlemi böyle sürdürürsek istediğimiz uzunlukta bir yazı elde etmiş oluruz.\n",
    "\n",
    "5) Modelin bu biçimde oluşturulması yazının tek düze ve yaratıcılık içermeyen bir yapıda olmasına yol açabilmektedir. Bu nedenle her dfefasında ağdan elde edilen karakteri doğrudan kullanmak yerine işin içine bir miktar rassallık da katarak belli bir olasılığı dağılımı çerçevesinde yeni karakterin elde edilmesi daha yaratıcı bir metnin oluşturulmasına yol açacaktır.\n",
    "\n",
    "Örnek: Belli bir bestecinin tarzına göre beste yapan bir sinir ağı modeli oluşturulabilir.\n",
    "\n",
    "Uygulamalı Örnek: Nietzsche 'nin metinleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AlreadyExistsError",
     "evalue": "Another metric with the same name already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAlreadyExistsError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m      3\u001b[0m path \u001b[38;5;241m=\u001b[39m tensorflow\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mget_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnietzsche.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://s3.amazonaws.com/text-datasets/nietzsche.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\tensorflow\\keras\\__init__.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constraints\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initializers\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\tensorflow\\keras\\estimator\\__init__.py:10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function \u001b[38;5;28;01mas\u001b[39;00m _print_function\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_to_estimator_v2 \u001b[38;5;28;01mas\u001b[39;00m model_to_estimator\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _print_function\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\estimator\\__init__.py:28\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Keras has undeclared dependency on tensorflow/estimator:estimator_py.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# As long as you depend //third_party/py/tensorflow:tensorflow target\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# everything will work as normal.\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m _model_to_estimator_usage_gauge \u001b[38;5;241m=\u001b[39m \u001b[43mmonitoring\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBoolGauge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tensorflow/api/keras/model_to_estimator\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWhether tf.keras.estimator.model_to_estimator() is called.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mversion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# LINT.IfChange\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeras.estimator.model_to_estimator\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_to_estimator\u001b[39m(\n\u001b[0;32m     36\u001b[0m     keras_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     metric_names_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     43\u001b[0m     export_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\monitoring.py:356\u001b[0m, in \u001b[0;36mBoolGauge.__init__\u001b[1;34m(self, name, description, *labels)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, description, \u001b[38;5;241m*\u001b[39mlabels):\n\u001b[0;32m    349\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new BoolGauge.\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m    *labels: The label list of the new metric.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 356\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBoolGauge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBoolGauge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_bool_gauge_methods\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\monitoring.py:131\u001b[0m, in \u001b[0;36mMetric.__init__\u001b[1;34m(self, metric_name, metric_methods, label_length, *args)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_methods):\n\u001b[0;32m    128\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot create \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m metric with label >= \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    129\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_name, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_methods)))\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metric_methods\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_label_length\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAlreadyExistsError\u001b[0m: Another metric with the same name already exists."
     ]
    }
   ],
   "source": [
    "import tensorflow.keras\n",
    "\n",
    "path = tensorflow.keras.utils.get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "\n",
    "with open(path) as f:\n",
    "    text = f.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "\n",
    "path = tensorflow.keras.utils.get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "\n",
    "with open(path) as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "print(text)\n",
    "\n",
    "# yazıda 3'er atlamalı olarak 60'ar karakterden oluşan parçaları sentences isimli bir listeye yerleştireceğiz.\n",
    "TEXT_LENGTH = 60\n",
    "STEP = 3\n",
    "\n",
    "# yapay sinir ağı 60 karakterden sonraki 61. karakteri tahmin edeceğinden dolayı bu 60 karakterden sonraki karakteri de next_char isimli bir listede toplayacağız.\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - TEXT_LENGTH, STEP):\n",
    "  sentences.append(text[i:i + TEXT_LENGTH])\n",
    "  next_chars.append(text[i + TEXT_LENGTH])\n",
    "\n",
    "print(\"Number of sequences: {}\".format(len(sentences)))\n",
    "\n",
    "# bundan sonra yazı içerisinde farklı kaç tane karakter olduğunu tespit edip bu karakterleri de bir listeye yerleştirerek\n",
    "# koda devam edelim.\n",
    "chars = sorted(list(set(text)))\n",
    "print('Number of unique chars: {}'.format(len(chars)))\n",
    "\n",
    "chars\n",
    "\n",
    "# Yazı parçalarındaki karakterlerinin her birini \"one hot encoding\" biçiminde oluşturmarmız gerekir. Zira eğer biz akrakter kodlarını\n",
    "# doğrudan kullanırsak model sanki sıralı (ordinal) bir ölçek söz konusuymuş gibi davranacaktır. OHE işlemini kolaylaştırmak için bir\n",
    "# sözlük oluşturabiliriz. Bu sözlük sayesinde yazı içerisindeki bir karakter verildiğinde onun indeksini hızlı bir biçimde elde edeceğiz.\n",
    "\n",
    "char_dict = {char: index for index, char in enumerate(chars)}\n",
    "\n",
    "char_dict\n",
    "\n",
    "# Bu sözlük sayesinde biz karakteri verdiğimizde ona karşı gelen sayısal değeri alabileceğiz. Artık sıra training_dataset_x ve\n",
    "# training_dataset_y nesnelerinin ndarray olarak oluşturulmasında. Bunun için önce bu diziler içnide sıfır olacak biçimde oluşturualım\n",
    "import numpy as np\n",
    "training_dataset_x = np.zeros((len(sentences), TEXT_LENGTH, len(chars)), dtype=np.uint8)\n",
    "training_dataset_y = np.zeros((len(sentences), len(chars)), dtype=np.uint8)\n",
    "\n",
    "# Yalnızca uygun elemanı 1 yaparak one hot encoding vektörlerini kolay bir biçimde oluştuırabiliriz.\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "  for k, char in enumerate(sentence):\n",
    "    training_dataset_x[i, k, char_dict[char]] = 1\n",
    "  training_dataset_y[i, char_dict[next_chars[i]]] = 1\n",
    "\n",
    "# artık training_dataset_x ve training_dataset_y one hot encoding biçiminde oluşturulmuş durumdadır.\n",
    "\n",
    "training_dataset_y\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "\n",
    "model = Sequential(name = 'Nietzsche-LSTM')\n",
    "model.add(LSTM(128, activation='tanh', input_shape=(TEXT_LENGTH, len(chars)), name = 'LSTM'))\n",
    "model.add(Dropout(0.3, name = 'Dropout'))\n",
    "model.add(Dense(len(chars), activation='softmax', name = 'Output'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Çıktı matnanının toplamda len(chars) uzunlukta nöronlardan oluştuğunu görüyoruz. Çıktı katmanının aktivasyon fonksiyonu \"softmax\"\n",
    "# olduuğna göre tüm çıktı nöronlarının toplam değeri 1 olacaktır. Tabii biz bunlar arasından en yükske değeri tahmin olarak alacağız.\n",
    "# Ağımızın optimizasyon algoritması \"rmsprop\" olarak loss fonksiyonun da daha önceden benzer sıınflandırma örneklerinde olduğu gibi\n",
    "# \"categorical_crossentropy\" olarak alınmıştır.\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "esc = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=20, validation_split=0.2, callbacks=[esc])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches((15, 5))\n",
    "plt.title('Loss - Epoch Graphics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(range(1, len(hist.history['loss']) + 1), hist.history['loss'])\n",
    "plt.plot(range(1, len(hist.history['val_loss']) + 1), hist.history['val_loss'])\n",
    "plt.legend(['Loss', 'Validation Loss'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches((15, 5))\n",
    "plt.title('Categorical Accuracy - Epoch Graphics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Categorical Accuracy')\n",
    "plt.plot(range(1, len(hist.history['categorical_accuracy']) + 1), hist.history['categorical_accuracy'])\n",
    "plt.plot(range(1, len(hist.history['val_categorical_accuracy']) + 1), hist.history['val_categorical_accuracy'])\n",
    "plt.legend(['Categorical Accuracy', 'Validation Categorical Accuracy'])\n",
    "plt.show()\n",
    "\n",
    "# Şimdi de belli bir TEXT_LENGTH uzunluuğnda yazı alıp bir döngü içerisinde ona 400 karakter ekleyerek yeni bir yazı üretelim.\n",
    "# TEXT_LENGTH uzunluğundaki yazıyı verip bundan elde edilecek TEXT_LENGTH + 1. karakteri tahmin ederiz. Sonra son TEXT_LENGTH uzunlukta\n",
    "# yazıyı verip sonraki karakteri tahmin ederiz. Böylece 400 karakteri bir döngü içerisinde bu biçimde elde ederiz. Örneğimizde ağ\n",
    "# tarafından elde edilen karkaterlerin olasılık dağılımına göre biraz değiştirilmesi gerebeiblmektedir. Yani üretilen karakterin değil de ona yakın\n",
    "# bir karakterin seçilmesi tekdüzeliği engellemek bakımından uygun olabilir.\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "  preds = np.asarray(preds).astype('float64')\n",
    "  preds = np.log(preds) / temperature\n",
    "  exp_preds = np.exp(preds)\n",
    "  preds = exp_preds / np.sum(exp_preds)\n",
    "  probas = np.random.multinomial(1, preds, 1)\n",
    "\n",
    "  return np.argmax(probas)\n",
    "\n",
    "# Programın yazı üretim kısmı için TEXT_LENGTH uzunluktaki yazıları ohe biçimine dönüştürmemiz gerekir.\n",
    "\n",
    "def str_to_ohe(s):\n",
    "  ohe = np.zeros(shape=(1, TEXT_LENGTH, len(chars)))\n",
    "  for index, char in enumerate(s):\n",
    "    ohe[0, index, char_dict[char]] = 1.0\n",
    "\n",
    "  return ohe\n",
    "\n",
    "# Şimdi rastgele bir TEXT_LENGTH uzunluuğunda bir yazı oluşturalım.\n",
    "\n",
    "r = np.random.randint(len(sentences) - TEXT_LENGTH - 1)\n",
    "random_initial_sentence = text[r: r + 60]\n",
    "\n",
    "r\n",
    "\n",
    "random_initial_sentence\n",
    "\n",
    "# Şimdi de 400 karakterlik bir yazı üretelim.\n",
    "\n",
    "generated_text = random_initial_sentence\n",
    "for i in range(400):\n",
    "  ohe = str_to_ohe(generated_text[i: TEXT_LENGTH + i])\n",
    "  result = model.predict(ohe)[0]\n",
    "  char = chars[sample(result, 0.2)]\n",
    "  generated_text += char\n",
    "\n",
    "print(f'[{generated_text[0:TEXT_LENGTH]}]{generated_text[TEXT_LENGTH:]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
